{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a5532a-325e-4224-a422-7e8679fcb6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "from torch.utils.data import DataLoader, ConcatDataset, WeightedRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import ast\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "for warn in [UserWarning, FutureWarning]: warnings.filterwarnings(\"ignore\", category = warn)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3292b4a0-d762-437f-bd58-4f09f2f3c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMambaBlock(nn.Module):\n",
    "    def __init__(self, d_input, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_input, d_model)\n",
    "        self.s_B = nn.Linear(d_model, d_model)\n",
    "        self.s_C = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_input)\n",
    "        self.norm = nn.LayerNorm(d_input)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        x = self.in_proj(x)\n",
    "        B = self.s_B(x)\n",
    "        C = self.s_C(x)\n",
    "        x = x + B + C\n",
    "        x = self.activation(x)\n",
    "        x = self.out_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norm(x + x_in)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54ebd3f-6f53-4662-8afe-67c659dfbd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.layer_2(x)\n",
    "class AddAndNorm(nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, residual):\n",
    "        return self.norm(x + self.dropout(residual))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[: x.size(1)].detach()  # Отключаем градиенты\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dropout=0.1, positional_encoding=False):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.self_attention = nn.MultiheadAttention(input_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.feed_forward = PositionWiseFeedForward(input_dim, input_dim, dropout=dropout)\n",
    "        self.add_norm_after_attention = AddAndNorm(input_dim, dropout=dropout)\n",
    "        self.add_norm_after_ff = AddAndNorm(input_dim, dropout=dropout)\n",
    "        self.positional_encoding = PositionalEncoding(input_dim) if positional_encoding else None\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        if self.positional_encoding:\n",
    "            key = self.positional_encoding(key)\n",
    "            value = self.positional_encoding(value)\n",
    "            query = self.positional_encoding(query)\n",
    "\n",
    "        attn_output, _ = self.self_attention(query, key, value, need_weights=False)\n",
    "\n",
    "        x = self.add_norm_after_attention(attn_output, query)\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.add_norm_after_ff(ff_output, x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8da44a-5ce2-4532-8c95-7fb9e0805ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionMamba(nn.Module):\n",
    "    def __init__(self, input_dim_emotion=1024, input_dim_personality=1024, hidden_dim=128, out_features=512, mamba_layer_number=2, mamba_d_model=256, positional_encoding=True, num_transformer_heads=4, transformer_dropout=0.1, tr_layer_number=1, dropout=0.1, num_emotions=7, num_traits=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.emo_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim_emotion, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.emotion_encoder = nn.ModuleList([\n",
    "            CustomMambaBlock(hidden_dim, mamba_d_model, dropout=dropout)\n",
    "            for _ in range(mamba_layer_number)\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.emotion_fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, out_features),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, num_emotions)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, emotion_input=None, personality_input=None, return_features=False):\n",
    "        emo = self.emo_proj(emotion_input)  # (B, T, hidden_dim)\n",
    "        for layer in self.emotion_encoder:\n",
    "            emo = layer(emo)\n",
    "        out_emo = self.emotion_fc_out(emo.mean(dim=1))  # (B, num_emotions)\n",
    "        if return_features:\n",
    "            return {\n",
    "                'emotion_logits': out_emo,\n",
    "                'last_encoder_features': emo,\n",
    "            }\n",
    "        else:\n",
    "            return {'emotion_logits': out_emo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad62b756-30ad-4d4a-91fb-ca710e561c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelWithAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim = 128, num_heads = 4, num_layers = 8, dropout = 0.1):\n",
    "        super(TransformerModelWithAttention, self).__init__()\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 1000, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = hidden_dim, nhead = num_heads, dim_feedforward = hidden_dim, dropout = dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers = num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = x + self.positional_encoding[:, :seq_len, :]\n",
    "        encoder_output = self.transformer_encoder(x).to(device)\n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33abb31f-f616-42ff-8d1c-68abb0cd9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionTransformer(nn.Module):\n",
    "    def __init__(self, input_dim_emotion=1024, input_dim_personality=1024, hidden_dim=128, out_features=512, per_activation=\"sigmoid\", positional_encoding=True, num_transformer_heads=4, tr_layer_number=1, dropout=0.1, num_emotions=7, num_traits=5, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.emo_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim_emotion, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.emotion_encoder = TransformerModelWithAttention(hidden_dim=hidden_dim, num_heads=num_transformer_heads, num_layers=tr_layer_number, dropout=dropout)\n",
    "\n",
    "        self.emotion_fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, out_features),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, num_emotions)\n",
    "        )\n",
    "\n",
    "    def forward(self, emotion_input=None, personality_input=None, return_features=False):\n",
    "        emo = self.emo_proj(emotion_input)\n",
    "\n",
    "        emo = self.emotion_encoder(emo)\n",
    "\n",
    "        out_emo = self.emotion_fc_out(emo.mean(dim=1))  # (B, num_emotions)\n",
    "        \n",
    "        if return_features:\n",
    "            return {\n",
    "                'emotion_logits': out_emo,\n",
    "                'last_encoder_features': emo,\n",
    "            }\n",
    "        else:\n",
    "            return {'emotion_logits': out_emo}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03597094-1406-4abd-8869-ab90ce8d7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalityTransformer(nn.Module):\n",
    "    def __init__(self, input_dim_emotion=512, input_dim_personality=512, hidden_dim=128, out_features=512, mamba_layer_number=2, mamba_d_model=256, per_activation=\"sigmoid\", positional_encoding=True, num_transformer_heads=4, tr_layer_number=1, dropout=0.1, num_emotions=7, num_traits=5, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.per_proj = nn.Sequential(\n",
    "            nn.Linear(input_dim_personality, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.personality_encoder = nn.ModuleList([\n",
    "            TransformerEncoderLayer(\n",
    "                input_dim=hidden_dim,\n",
    "                num_heads=num_transformer_heads,\n",
    "                dropout=dropout,\n",
    "                positional_encoding=positional_encoding\n",
    "            ) for _ in range(tr_layer_number)\n",
    "        ])\n",
    "\n",
    "        self.personality_fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, out_features),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, num_traits)\n",
    "        )\n",
    "\n",
    "        if per_activation == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif per_activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, emotion_input=None, personality_input=None, return_features=False):\n",
    "        per = self.per_proj(personality_input)\n",
    "\n",
    "        for layer in self.personality_encoder:\n",
    "            per += layer(per, per, per)\n",
    "\n",
    "        out_per = self.personality_fc_out(per.mean(dim=1))\n",
    "        \n",
    "        if return_features:\n",
    "            return {\n",
    "                'personality_scores': self.activation(out_per),\n",
    "                'last_encoder_features': per,\n",
    "            }\n",
    "        else:\n",
    "            return {'personality_scores': self.activation(out_per)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b7a1a3f-448f-4c9a-8381-dc832ed6c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, emo_model, per_model, input_dim_emotion=512, input_dim_personality=512, hidden_dim=128, out_features=512, mamba_layer_number=2, mamba_d_model=256, per_activation=\"sigmoid\", positional_encoding=True, num_transformer_heads=4, tr_layer_number=1, dropout=0.1, num_emotions=7, num_traits=5, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.emo_model = emo_model\n",
    "        self.per_model = per_model\n",
    "\n",
    "        for param in self.emo_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.per_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.emo_proj = nn.Sequential(\n",
    "            nn.Linear(self.emo_model.hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.per_proj = nn.Sequential(\n",
    "            nn.Linear(self.per_model.hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.emotion_to_personality_attn = nn.ModuleList([\n",
    "            TransformerEncoderLayer(\n",
    "                input_dim=hidden_dim,\n",
    "                num_heads=num_transformer_heads,\n",
    "                dropout=dropout,\n",
    "                positional_encoding=positional_encoding\n",
    "            ) for _ in range(tr_layer_number)\n",
    "        ])\n",
    "\n",
    "        self.personality_to_emotion_attn = nn.ModuleList([\n",
    "            TransformerEncoderLayer(\n",
    "                input_dim=hidden_dim,\n",
    "                num_heads=num_transformer_heads,\n",
    "                dropout=dropout,\n",
    "                positional_encoding=positional_encoding\n",
    "            ) for _ in range(tr_layer_number)\n",
    "        ])\n",
    "\n",
    "        self.emotion_personality_fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, out_features),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, num_emotions)\n",
    "        )\n",
    "\n",
    "        self.personality_emotion_fc_out = nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2, out_features),\n",
    "            nn.LayerNorm(out_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(out_features, num_traits)\n",
    "        )        \n",
    "\n",
    "        if per_activation == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif per_activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, emotion_input=None, personality_input=None, return_features=False):\n",
    "        emo_features = self.emo_model(emotion_input=emotion_input, return_features=True)\n",
    "        per_features = self.per_model(personality_input=personality_input, return_features=True)\n",
    "\n",
    "        emo_emd = self.emo_proj(emo_features['last_encoder_features'])\n",
    "        per_emd = self.per_proj(per_features['last_encoder_features'])\n",
    "        \n",
    "        # padding\n",
    "        max_len = max(emo_emd.shape[1], per_emd.shape[1])\n",
    "        emo_emd = emo_emd.cpu().detach().numpy()\n",
    "        per_emd = per_emd.cpu().detach().numpy()\n",
    "        emo_emd = np.pad(emo_emd[:, :max_len, :], ((0, 0), (0, max(0, max_len - emo_emd.shape[1])), (0, 0)), \"constant\")\n",
    "        per_emd = np.pad(per_emd[:, :max_len, :], ((0, 0), (0, max(0, max_len - per_emd.shape[1])), (0, 0)), \"constant\")\n",
    "        emo_emd = torch.tensor(emo_emd, device=self.device)\n",
    "        per_emd = torch.tensor(per_emd, device=self.device)\n",
    "\n",
    "        for layer in self.emotion_to_personality_attn:\n",
    "            emo_emd += layer(emo_emd, per_emd, per_emd)\n",
    "\n",
    "        for layer in self.personality_to_emotion_attn:\n",
    "            per_emd += layer(per_emd, emo_emd, emo_emd)\n",
    "\n",
    "        fused = torch.cat([emo_emd, per_emd], dim=-1)\n",
    "        emotion_logits = self.emotion_personality_fc_out(fused.mean(dim=1))\n",
    "        personality_scores = self.personality_emotion_fc_out(fused.mean(dim=1))\n",
    "\n",
    "        if return_features:\n",
    "            return {\n",
    "                'emotion_logits': (emotion_logits+emo_features['emotion_logits'])/2,\n",
    "                'personality_scores': (self.activation(personality_scores)+per_features['personality_scores'])/2,\n",
    "                'last_emo_encoder_features': emo_emd,\n",
    "                'last_per_encoder_features': per_emd,\n",
    "            }\n",
    "        else:\n",
    "            return {'emotion_logits': (emotion_logits+emo_features['emotion_logits'])/2,\n",
    "                    'personality_scores': (self.activation(personality_scores)+per_features['personality_scores'])/2,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594b9946-2588-4a0d-b7a7-964cb1472558",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9eb1eb8-3536-43a2-961a-d2ca61fe0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_matrix(matrix):\n",
    "    threshold1 = 1 - 1/7 \n",
    "    threshold2 = 1/7\n",
    "    mask1 = matrix[:, 0] >= threshold1\n",
    "    result = np.zeros_like(matrix[:, 1:])\n",
    "    transformed = (matrix[:, 1:] >= threshold2).astype(int)\n",
    "    result[~mask1] = transformed[~mask1]\n",
    "    return result\n",
    "def process_predictions(pred_emo):\n",
    "    pred_emo = torch.nn.functional.softmax(pred_emo, dim=1).cpu().detach().numpy()\n",
    "    pred_emo = transform_matrix(pred_emo).tolist()\n",
    "    return pred_emo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ad79c-0c9e-45a9-8324-c6a5a878b65c",
   "metadata": {},
   "source": [
    "### Load emotion classifier and personality regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4366b6df-5ab1-44cd-a064-9843337f1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_path_transformer = os.path.join(\".\", \"best_models\", \"Transformer_bge-small_emotion.pt\")\n",
    "emo_path_mamba_transformer = os.path.join(\".\", \"best_models\", \"Mamba_bge-small_emotion.pt\")\n",
    "pers_path = os.path.join(\".\", \"best_models\", \"Transformer_bge-small_personality.pt\")\n",
    "fusion_path_transformer = os.path.join(\".\", \"best_models\", \"Transformer_bge-small_fusion.pt\")\n",
    "fusion_path_mamba_transformer = os.path.join(\".\", \"best_models\", \"Mamba_Transformer_bge-small_fusion.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72a7414-3054-477e-b928-0365f0e70c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(emo_path, pers_path, fusion_path, device, mode='transformer'):\n",
    "    per_model = PersonalityTransformer(input_dim_emotion=384, input_dim_personality=384, hidden_dim=512, out_features=128, num_transformer_heads=8, tr_layer_number=1, dropout=0.2).to(device)\n",
    "    checkpoint = torch.load(pers_path, map_location=device)\n",
    "    per_model.load_state_dict(checkpoint)\n",
    "    \n",
    "    if mode == 'mamba+transformer':\n",
    "        emo_model = EmotionMamba(input_dim_emotion=384, input_dim_personality=384, hidden_dim=256, out_features=256, mamba_layer_number=4, mamba_d_model=256, dropout=0.2).to(device)\n",
    "        checkpoint = torch.load(emo_path, map_location=device)\n",
    "        emo_model.load_state_dict(checkpoint)\n",
    "        model = FusionTransformer(emo_model, per_model, input_dim_emotion=384, input_dim_personality=384, hidden_dim=256, out_features=256, tr_layer_number=2, num_transformer_heads=4).to(device)\n",
    "        checkpoint = torch.load(fusion_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint)\n",
    "    else:\n",
    "        emo_model = EmotionTransformer(input_dim_emotion=384, input_dim_personality=384, hidden_dim=256, out_features=256, num_transformer_heads=4, tr_layer_number=3, dropout=0).to(device)\n",
    "        checkpoint = torch.load(emo_path, map_location=device)\n",
    "        emo_model.load_state_dict(checkpoint)\n",
    "        model = FusionTransformer(emo_model, per_model, input_dim_emotion=384, input_dim_personality=384, hidden_dim=256, out_features=512, tr_layer_number=1, num_transformer_heads=4).to(device)\n",
    "        checkpoint = torch.load(fusion_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f37feac-ffae-4777-931a-3ae7340d0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text, device):\n",
    "    feature_extractor_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "    feature_extractor_model = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\").to(device)\n",
    "    encoded_input = feature_extractor_tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor_model(**encoded_input)[0][0]\n",
    "    model.eval()\n",
    "    return model(emotion_input=features.unsqueeze(0).to(device), personality_input=features.unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2c504-ce5e-4155-b246-9b4608075fa0",
   "metadata": {},
   "source": [
    "### Transformers for emotion and personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41cd9188-a95b-4122-96d9-d12da8400f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_models(emo_path_transformer, pers_path, fusion_path_transformer, device, 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628fb222-8df1-4157-8b69-359f5e850f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = inference(model, 'You are the best!', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ce520f8-496c-44e4-8e05-bc3fdc309bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion\n",
      "  Neutral: 0.1069\n",
      "  Anger: 0.0940\n",
      "  Disgust: 0.0167\n",
      "  Fear: 0.0921\n",
      "  Happiness: 0.6003\n",
      "  Sadness: 0.0492\n",
      "  Surprise: 0.0409\n",
      "Personality\n",
      "  Openness: 0.6993\n",
      "  Conscientiousness: 0.6570\n",
      "  Extraversion: 0.6099\n",
      "  Agreeableness: 0.6531\n",
      "  Neuroticism: 0.6955\n"
     ]
    }
   ],
   "source": [
    "print(\"Emotion\")\n",
    "prob_emo = torch.nn.functional.softmax(logits['emotion_logits'], dim=1).cpu().detach().numpy()\n",
    "emo_names = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
    "for name, v in zip(emo_names, prob_emo[0]):\n",
    "    print(f\"  {name}: {v:.4f}\")\n",
    "print(\"Personality\")   \n",
    "pers_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "for name, v in zip(pers_names, logits['personality_scores'].tolist()[0]):\n",
    "    print(f\"  {name}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0aef1e-ac14-4d8f-b65b-e2520363fb7a",
   "metadata": {},
   "source": [
    "### Mamba for emotion and Transformer for personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9f5fd6f-43a6-4b3b-84a4-4e6da61c4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_models(emo_path_mamba_transformer, pers_path, fusion_path_mamba_transformer, device, 'mamba+transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2ee0fc1-8ae0-419f-b22c-d840513e8a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = inference(model, 'You are the best!', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d660983-9dd2-40b1-9f19-4833944b54b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion\n",
      "  Neutral: 0.0885\n",
      "  Anger: 0.0886\n",
      "  Disgust: 0.0063\n",
      "  Fear: 0.0963\n",
      "  Happiness: 0.6483\n",
      "  Sadness: 0.0283\n",
      "  Surprise: 0.0436\n",
      "Personality\n",
      "  Openness: 0.7194\n",
      "  Conscientiousness: 0.6264\n",
      "  Extraversion: 0.6585\n",
      "  Agreeableness: 0.6385\n",
      "  Neuroticism: 0.6923\n"
     ]
    }
   ],
   "source": [
    "print(\"Emotion\")\n",
    "prob_emo = torch.nn.functional.softmax(logits['emotion_logits'], dim=1).cpu().detach().numpy()\n",
    "emo_names = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
    "for name, v in zip(emo_names, prob_emo[0]):\n",
    "    print(f\"  {name}: {v:.4f}\")\n",
    "print(\"Personality\")   \n",
    "pers_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "for name, v in zip(pers_names, logits['personality_scores'].tolist()[0]):\n",
    "    print(f\"  {name}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3f223-bbfb-4644-8c22-56772ebe66a9",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd7136df-70bd-427f-a02c-9eb1caa7918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cmu_mosei_data(path, part='train'):\n",
    "    if part in ['train', 'dev', 'test']:\n",
    "        df = pd.read_csv(os.path.join(path, part + \"_full.csv\"))\n",
    "    else:\n",
    "        raise ValueError('Unknown part of train / dev / test')\n",
    "    return df.text.values, np.dstack((df.Neutral.to_numpy(), df.Anger.to_numpy(), df.Disgust.to_numpy(), df.Fear.to_numpy(), df.Happiness.to_numpy(), df.Sadness.to_numpy(), df.Surprise.to_numpy()))\n",
    "\n",
    "def get_first_imp_data(path, part='train'):\n",
    "    if part in ['train', 'dev', 'test']:\n",
    "        df = pd.read_csv(os.path.join(path, \"FIv2_\" + part + \".csv\"))\n",
    "        df = df.fillna(\"\")\n",
    "    else:\n",
    "        raise ValueError('Unknown part of train / dev / test')\n",
    "    return df.text.values, np.dstack((df.openness.to_numpy(), df.conscientiousness.to_numpy(), df.extraversion.to_numpy(), df.agreeableness.to_numpy(), df['non-neuroticism'].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74df8533-9d9f-46bf-b4ec-51fdf803756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetEmotionPersonality(Dataset): \n",
    "    def __init__(self, dataset='CMU-MOSEI', part='train', path='data', path_to_emb=None, model='jina'): \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if dataset == 'CMU-MOSEI':\n",
    "            texts, labels = get_cmu_mosei_data(path, part)\n",
    "        elif dataset == 'FirstImpressionV2':\n",
    "            texts, labels = get_first_imp_data(path, part)\n",
    "        else:\n",
    "            raise ValueError('Unknown dataset (CMU-MOSEI / FirstImpressionV2)')\n",
    "        self.x = texts\n",
    "        self.y = labels[0]\n",
    "        if path_to_emb is None:\n",
    "            if model == 'jina':\n",
    "                self.feature_extractor_tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True)\n",
    "                self.feature_extractor_model = AutoModel.from_pretrained(\"jinaai/jina-embeddings-v3\", code_revision='da863dd04a4e5dce6814c6625adfba87b83838aa', trust_remote_code=True).to(self.device)\n",
    "            elif model == 'xlm-roberta-base':\n",
    "                self.feature_extractor_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "                self.feature_extractor_model = AutoModel.from_pretrained('xlm-roberta-base').to(self.device)\n",
    "            elif model == 'bge-small':\n",
    "                self.feature_extractor_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "                self.feature_extractor_model = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\").to(self.device)\n",
    "            else:\n",
    "                raise ValueError('Unknown name of model')\n",
    "            self.text_embedding = []\n",
    "            for t in tqdm(texts):\n",
    "                encoded_input = self.feature_extractor_tokenizer(t, padding=True, truncation=True, return_tensors='pt').to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    features = self.feature_extractor_model(**encoded_input)[0][0]\n",
    "                self.text_embedding.append(features)\n",
    "                \n",
    "        else:\n",
    "            with open(path_to_emb, 'rb') as file:\n",
    "                self.text_embedding = pickle.load(file)\n",
    "        self.n_samples = len(texts)        \n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        return {\n",
    "            \"text\": self.x[index], \n",
    "            \"text_embedding\" : self.text_embedding[index],\n",
    "            \"label\" :self.y[index] \n",
    "        }\n",
    "        \n",
    "    def __len__(self): \n",
    "        return self.n_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16b0c62d-39c3-443a-bcc1-cd49e43c180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Собирает список образцов в единый батч, отбрасывая None (невалидные).\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch = [x for x in batch if x is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    text = [b[\"text\"] for b in batch]\n",
    "\n",
    "    labels = [b[\"label\"] for b in batch]\n",
    "    label_tensor = torch.tensor(labels, device=device)\n",
    "\n",
    "    text_embedding = [torch.tensor(b[\"text_embedding\"], device=device) for b in batch]\n",
    "    text_tensor = pad_sequence(text_embedding, batch_first=True)\n",
    "    text_tensor = text_tensor.to(device)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"text_embedding\": text_tensor.float(),\n",
    "        \"label\": label_tensor,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16f508b7-1882-4e31-8cd6-2932fe246470",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_test_dataset = dataset=DatasetEmotionPersonality(dataset='CMU-MOSEI', part='test', path_to_emb='bge-small_emotion_test_embedding.pkl')\n",
    "personality_test_dataset = dataset=DatasetEmotionPersonality(dataset='FirstImpressionV2', part='test', path_to_emb='bge-small_personality_test_embedding.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9219c031-3449-41df-b4c4-bb130d30f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "emotion_test_dataloader = DataLoader(dataset=emotion_test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
    "personality_test_dataloader = DataLoader(dataset=personality_test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate_fn)\n",
    "test_loaders = {'cmu_mosei' : emotion_test_dataloader, 'fiv2' : personality_test_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d9ddb61-38f8-4dc1-a878-64a50410a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.measures import uar, mf1, acc_func, ccc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52c941f8-08e1-4591-bd2d-d61965d40fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_predictions(pred_emo, true_emo):\n",
    "    pred_emo = torch.nn.functional.softmax(pred_emo, dim=1).cpu().detach().numpy()\n",
    "    pred_emo = transform_matrix(pred_emo).tolist()\n",
    "    true_emo = true_emo.cpu().detach().numpy()\n",
    "    true_emo = np.where(true_emo > 0, 1, 0)[:, 1:].tolist()\n",
    "    return pred_emo, true_emo\n",
    "\n",
    "def run_emo_eval(model, loader, device=\"cuda\", mode = \"emotion\", disable_print=True):\n",
    "    \"\"\"\n",
    "    Оценка модели по задаче эмоций. Возвращает (uar, mf1).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, disable=disable_print):\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            labels = batch[\"label\"].to(device)      # shape: (B, 7)\n",
    "            text  = batch[\"text_embedding\"].to(device)     # shape: (B, D, F)\n",
    "            \n",
    "\n",
    "            if mode == \"emotion\":\n",
    "                logits = model(emotion_input=text)\n",
    "            elif mode == \"fusion\":\n",
    "                logits = model(emotion_input=text, personality_input=text)\n",
    "\n",
    "            bs = text.shape[0]\n",
    "            total += bs\n",
    "\n",
    "            preds, target =  process_predictions(logits['emotion_logits'], labels)\n",
    "            total_preds.extend(preds)\n",
    "            total_targets.extend(target)\n",
    "\n",
    "    uar_m = uar(total_targets, total_preds)\n",
    "    mf1_m = mf1(total_targets, total_preds)\n",
    "\n",
    "    return uar_m, mf1_m\n",
    "\n",
    "def run_per_eval(model, loader, device=\"cuda\", mode=\"personality\", disable_print=True):\n",
    "    \"\"\"\n",
    "    Оценка модели по задаче персональные качества личности. Возвращает (m_acc, m_ccc).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_preds = []\n",
    "    total_targets = []\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, disable=disable_print):\n",
    "            if batch is None:\n",
    "                continue\n",
    "\n",
    "            labels = batch[\"label\"].to(device)      # shape: (B, 7)\n",
    "            text  = batch[\"text_embedding\"].to(device)      # shape: (B, D, F)\n",
    "            if mode == \"personality\":\n",
    "                logits = model(personality_input=text)\n",
    "            elif mode == \"fusion\":\n",
    "                logits = model(emotion_input=text, personality_input=text)\n",
    "\n",
    "            bs = text.shape[0]\n",
    "            total += bs\n",
    "\n",
    "            preds = logits['personality_scores']\n",
    "            total_preds.extend(preds.detach().cpu().numpy())\n",
    "            total_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    total_preds = np.array(total_preds)\n",
    "    total_targets = np.array(total_targets)\n",
    "\n",
    "    m_acc = acc_func(total_targets, total_preds)\n",
    "    m_ccc = ccc(total_targets, total_preds)\n",
    "\n",
    "    return m_acc, m_ccc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25439f2-ca47-4383-9530-e3ab967c55e6",
   "metadata": {},
   "source": [
    "### Transformers for emotion and personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d4c11d8-dc20-4ba3-b4cd-0b4e7e807097",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_models(emo_path_transformer, pers_path, fusion_path_transformer, device, 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2e16786-dba9-4b6d-aef3-5fe8cab2d56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:44<00:00,  3.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.647993607126586, 0.5922498588027434)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_emo_eval(model, test_loaders['cmu_mosei'], device=\"cuda\", mode = \"fusion\", disable_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab9456b9-8ca7-41b0-922d-e7a73bd33a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:22<00:00,  2.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8887264782277047, 0.2994060998941632)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_per_eval(model, test_loaders['fiv2'], device=\"cuda\", mode = \"fusion\", disable_print=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ebea7-bf9f-43cf-9693-11b6b29c9a20",
   "metadata": {},
   "source": [
    "### Mamba for emotion and Transformer for personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b9594db-9c8f-47ef-b207-22f141e0d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_models(emo_path_mamba_transformer, pers_path, fusion_path_mamba_transformer, device, 'mamba+transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20f152a7-1ede-4ac3-85f8-b995a8192b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:51<00:00,  2.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6481032578686216, 0.5847542245726806)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_emo_eval(model, test_loaders['cmu_mosei'], device=\"cuda\", mode = \"fusion\", disable_print=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edb8bc18-b3b4-4430-96b7-e7265c25b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:27<00:00,  2.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.888321585742975, 0.30968351651794035)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_per_eval(model, test_loaders['fiv2'], device=\"cuda\", mode = \"fusion\", disable_print=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google Colab Analog 2024 (PyTorch 2.5.1 + TensorFlow 2.18) [python-google_colab_gpu_2024]",
   "language": "python",
   "name": "conda-env-python-google_colab_gpu_2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
