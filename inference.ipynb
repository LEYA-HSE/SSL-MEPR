{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-05T02:32:54.700924Z",
     "start_time": "2025-07-05T02:32:49.451049Z"
    }
   },
   "outputs":[],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "\n",
    "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]  # (batch_size, sequence_length, hidden_size)\n",
    "        return hidden_states\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "audio_embedder = EmotionModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def process_audio(signal: np.ndarray, sampling_rate: int) -> np.ndarray:\n",
    "    inputs = processor(signal, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = audio_embedder(input_values)\n",
    "        embeddings = outputs\n",
    "        \n",
    "    return embeddings.detach().cpu().numpy()\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "    \n",
    "class PositionalEncoding_per(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_size, d_model, num_layers, num_heads, dim_feedforward, dropout, num_targets):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding_per(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.head = nn.Linear(d_model, num_targets)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_enc(x)\n",
    "        x = x.transpose(0, 1) \n",
    "        lengths_cpu = lengths.cpu()\n",
    "        mask = self._generate_padding_mask(lengths_cpu, x.size(0)).to(x.device)\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "        x = x.transpose(0, 1)\n",
    "        pooled = []\n",
    "        for i, L in enumerate(lengths_cpu):\n",
    "            if L > 0:\n",
    "                pooled.append(x[i, :L].mean(dim=0))\n",
    "            else:\n",
    "                pooled.append(torch.zeros(x.size(-1), device=x.device))\n",
    "        pooled = torch.stack(pooled, dim=0)\n",
    "        return self.head(pooled)\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_padding_mask(lengths, max_len):\n",
    "        mask = torch.arange(max_len).expand(len(lengths), max_len) >= lengths.unsqueeze(1)\n",
    "        return mask\n",
    "\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 num_layers: int,\n",
    "                 dim_feedforward: int,\n",
    "                 num_classes: int,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x)                      \n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        x = x.transpose(0, 1)\n",
    "        mask = self._generate_padding_mask(lengths, x.size(0))\n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        x = x.transpose(0, 1)         \n",
    "        pooled = torch.stack([\n",
    "            x[i, :lengths[i], :].mean(dim=0) if lengths[i] > 0\n",
    "            else torch.zeros(x.size(2), device=x.device)\n",
    "            for i in range(x.size(0))\n",
    "        ], dim=0)\n",
    "        return self.classifier(self.dropout(pooled))\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_padding_mask(lengths, max_len):\n",
    "        bs = lengths.size(0)\n",
    "        mask = torch.arange(max_len, device=lengths.device).expand(bs, max_len)\n",
    "        return mask >= lengths.unsqueeze(1)\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, emo_enc, per_enc, config):\n",
    "        super().__init__()\n",
    "        for p in emo_enc.parameters(): p.requires_grad = False\n",
    "        for p in per_enc.parameters(): p.requires_grad = False\n",
    "\n",
    "        self.emo_enc = emo_enc\n",
    "        self.per_enc = per_enc\n",
    "\n",
    "        h = config['hidden_dim']\n",
    "        d = config['dropout']\n",
    "        heads = config['tr_heads']\n",
    "\n",
    "        self.emo_proj = nn.Sequential(\n",
    "            nn.Linear(emo_enc.output_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(d)\n",
    "        )\n",
    "        self.per_proj = nn.Sequential(\n",
    "            nn.Linear(per_enc.output_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(d)\n",
    "        )\n",
    "\n",
    "        self.mha_e2p = nn.MultiheadAttention(embed_dim=h, num_heads=heads, dropout=d, batch_first=True)\n",
    "        self.mha_p2e = nn.MultiheadAttention(embed_dim=h, num_heads=heads, dropout=d, batch_first=True)\n",
    "\n",
    "        self.emo_head = nn.Sequential(\n",
    "            nn.Linear(h*2,   h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(d),\n",
    "            nn.Linear(h, config['num_emotions'])\n",
    "        )\n",
    "        self.per_head = nn.Sequential(\n",
    "            nn.Linear(h*2,   h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(d),\n",
    "            nn.Linear(h, config['num_traits'])\n",
    "        )\n",
    "\n",
    "    def forward(self, emo_input=None, per_input=None):\n",
    "        base_emo_logits = base_per_scores = None\n",
    "        if emo_input is not None:\n",
    "            x_e, len_e = emo_input\n",
    "            feat_e = self.emo_enc.extract_features(x_e, len_e)  \n",
    "            base_emo_logits = self.emo_enc(x_e, len_e)        \n",
    "            emo_emd = self.emo_proj(feat_e)                     \n",
    "        if per_input is not None:\n",
    "            x_p, len_p = per_input\n",
    "            feat_p = self.per_enc.extract_features(x_p, len_p)  \n",
    "            base_per_scores = self.per_enc(x_p, len_p)      \n",
    "            per_emd = self.per_proj(feat_p)                   \n",
    "\n",
    "        if emo_input is not None and per_input is not None:\n",
    "            attn_e2p, _ = self.mha_e2p(query=emo_emd, key=per_emd, value=per_emd)\n",
    "            emo_emd = emo_emd + attn_e2p\n",
    "\n",
    "            attn_p2e, _ = self.mha_p2e(query=per_emd, key=emo_emd, value=emo_emd)\n",
    "            per_emd = per_emd + attn_p2e\n",
    "\n",
    "            fe = emo_emd.mean(dim=1)  \n",
    "            fp = per_emd.mean(dim=1)  \n",
    "            cat = torch.cat([fe, fp], dim=-1)  \n",
    "\n",
    "            emo_new = self.emo_head(cat) \n",
    "            per_new = self.per_head(cat) \n",
    "\n",
    "            final_emo = (emo_new + base_emo_logits) / 2\n",
    "            final_per = (per_new + base_per_scores) / 2\n",
    "\n",
    "            return {'emotion': final_emo, 'personality': final_per}\n",
    "\n",
    "        elif emo_input is not None:\n",
    "            return {'emotion': base_emo_logits}\n",
    "        else:\n",
    "            return {'personality': base_per_scores}\n",
    "\n",
    "\n",
    "def load_pretrained_emotion_encoder(path, device):\n",
    "    enc = TransformerClassifier(\n",
    "        input_dim=1024,\n",
    "        d_model=128,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "        dim_feedforward=512,\n",
    "        num_classes=7,\n",
    "        dropout=0.2\n",
    "    ).to(device)\n",
    "\n",
    "    ck = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(ck)\n",
    "    enc.eval()\n",
    "\n",
    "    enc.output_dim = enc.input_proj.out_features  \n",
    "\n",
    "    def extract_features(x, lengths):\n",
    "        h = enc.input_proj(x)           \n",
    "        h = enc.pos_enc(h)          \n",
    "        h = h.transpose(0, 1)          \n",
    "        mask = TransformerClassifier._generate_padding_mask(lengths, h.size(0))\n",
    "        h = enc.transformer(h, src_key_padding_mask=mask) \n",
    "        h = h.transpose(0, 1)          \n",
    "        return h\n",
    "\n",
    "    enc.extract_features = extract_features\n",
    "    return enc\n",
    "\n",
    "\n",
    "def load_pretrained_personality_encoder(path, device):\n",
    "    enc = TransformerRegressor(\n",
    "        input_size=1024,\n",
    "        d_model=256,\n",
    "        num_heads=4,\n",
    "        num_layers=3,\n",
    "        dim_feedforward=512,\n",
    "        num_targets=5,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "\n",
    "    ck = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(ck)\n",
    "    enc.eval()\n",
    "\n",
    "    enc.output_dim = enc.input_proj.out_features  \n",
    "\n",
    "    def extract_features(x, lengths):\n",
    "        h = enc.input_proj(x)          \n",
    "        h = enc.pos_enc(h)\n",
    "        h = h.transpose(0, 1)          \n",
    "        mask = TransformerRegressor._generate_padding_mask(lengths.cpu(), h.size(0)).to(h.device)\n",
    "        h = enc.encoder(h, src_key_padding_mask=mask) \n",
    "        h = h.transpose(0, 1)          \n",
    "        return h\n",
    "\n",
    "    enc.extract_features = extract_features\n",
    "    return enc\n",
    "\n",
    "def load_fusion_model(\n",
    "    fusion_ckpt_path: str,\n",
    "    emo_encoder_ckpt: str,\n",
    "    per_encoder_ckpt: str,\n",
    "    device: str = 'cpu'\n",
    "):\n",
    "    device = torch.device(device)\n",
    "    emo_enc = load_pretrained_emotion_encoder(emo_encoder_ckpt, device)\n",
    "    per_enc = load_pretrained_personality_encoder(per_encoder_ckpt, device)\n",
    "    ckpt       = torch.load(fusion_ckpt_path, map_location=device)\n",
    "    best_cfg   = ckpt['config']\n",
    "    state_dict = ckpt['state_dict']\n",
    "    model = FusionTransformer(emo_enc, per_enc, best_cfg).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model, device\n",
    "\n",
    "def run_inference(\n",
    "    model: FusionTransformer,\n",
    "    device: torch.device,\n",
    "    embedding: np.ndarray\n",
    "):\n",
    "    if embedding.ndim == 3 and embedding.shape[0] == 1:\n",
    "        emb = embedding[0]  \n",
    "    elif embedding.ndim == 2:\n",
    "        emb = embedding    \n",
    "\n",
    "    x = torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(device) \n",
    "    lengths = torch.tensor([emb.shape[0]], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(emo_input=(x, lengths), per_input=(x, lengths))\n",
    "        emo_logits = out['emotion'].cpu().numpy().squeeze(0)\n",
    "        per_scores = out['personality'].cpu().numpy().squeeze(0)\n",
    "        emo_probs  = torch.softmax(torch.tensor(emo_logits), dim=-1).numpy()\n",
    "    return emo_probs, per_scores\n",
    "\n",
    "def extract_embeddings(audio_path: str) -> np.ndarray:\n",
    "        signal, sr = librosa.load(audio_path, sr=16000)\n",
    "        emb = process_audio(signal, sr)\n",
    "        return emb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FUSION_CKPT = \"best_fusion_overall_trans.pt\"\n",
    "    EMO_ENC_CKPT = \"final_best_model_uni_trans.pt\"\n",
    "    PER_ENC_CKPT = \"best_trans_fiv2.pt\"\n",
    "\n",
    "    model, device = load_fusion_model(\n",
    "        FUSION_CKPT, EMO_ENC_CKPT, PER_ENC_CKPT, device='cpu'\n",
    "    )\n",
    "    \n",
    "    emo_names = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
    "    pers_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "    \n",
    "    emb = extract_embeddings(\"your_audio.wav\")\n",
    "    emo_probs, per_scores = run_inference(model, device, emb)\n",
    "    print(\"Emotion\")\n",
    "    for name, v in zip(emo_names, emo_probs):\n",
    "        print(f\"  {name}: {v:.4f}\")\n",
    "        \n",
    "    print(\"Personality\")   \n",
    "    for name, v in zip(pers_names, per_scores):\n",
    "        print(f\"  {name}: {v:.4f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "outputs":[],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")\n",
    "\n",
    "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]  # (batch_size, sequence_length, hidden_size)\n",
    "        return hidden_states\n",
    "    \n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "audio_embedder = EmotionModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def process_audio(signal: np.ndarray, sampling_rate: int) -> np.ndarray:\n",
    "    inputs = processor(signal, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs[\"input_values\"].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = audio_embedder(input_values)\n",
    "        embeddings = outputs\n",
    "        \n",
    "    return embeddings.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "class CustomMambaBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_model, d_model)\n",
    "        self.s_B = nn.Linear(d_model, d_model)\n",
    "        self.s_C = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        x = self.in_proj(x)\n",
    "        x = x + self.s_B(x) + self.s_C(x)\n",
    "        x = self.act(x)\n",
    "        x = self.out_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.norm(x + x_in)\n",
    "\n",
    "class CustomMambaClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1024, d_model=256, num_layers=2, num_classes=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        self.blocks = nn.ModuleList([CustomMambaBlock(d_model, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x)\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        pooled = [x[i, :L].mean(dim=0) if L>0 else torch.zeros(x.size(-1), device=x.device)\n",
    "                  for i, L in enumerate(lengths)]\n",
    "        return self.fc(torch.stack(pooled, dim=0))\n",
    "    \n",
    "\n",
    "class CustomMambaRegressor(nn.Module):\n",
    "    def __init__(self, input_size, d_model, num_layers, num_targets, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "        self.blocks = nn.ModuleList([CustomMambaBlock(d_model, dropout) for _ in range(num_layers)])\n",
    "        self.head = nn.Linear(d_model, num_targets)\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.input_proj(x)\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        pooled = [x[i, :L].mean(dim=0) if L>0 else torch.zeros(x.size(-1), device=x.device)\n",
    "                  for i, L in enumerate(lengths)]\n",
    "        return self.head(torch.stack(pooled, dim=0))\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, emo_enc, per_enc, config):\n",
    "        super().__init__()\n",
    "       \n",
    "        for p in emo_enc.parameters(): p.requires_grad = False\n",
    "        for p in per_enc.parameters(): p.requires_grad = False\n",
    "\n",
    "        self.emo_enc = emo_enc\n",
    "        self.per_enc = per_enc\n",
    "\n",
    "        h = config['hidden_dim']\n",
    "        d = config['dropout']\n",
    "        heads = config['tr_heads']\n",
    "\n",
    "        \n",
    "        self.emo_proj = nn.Sequential(\n",
    "            nn.Linear(emo_enc.output_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(d)\n",
    "        )\n",
    "        self.per_proj = nn.Sequential(\n",
    "            nn.Linear(per_enc.output_dim, h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.Dropout(d)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.mha_e2p = nn.MultiheadAttention(embed_dim=h, num_heads=heads, dropout=d, batch_first=True)\n",
    "        self.mha_p2e = nn.MultiheadAttention(embed_dim=h, num_heads=heads, dropout=d, batch_first=True)\n",
    "\n",
    "  \n",
    "        self.emo_head = nn.Sequential(\n",
    "            nn.Linear(h*2,   h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(d),\n",
    "            nn.Linear(h, config['num_emotions'])\n",
    "        )\n",
    "        self.per_head = nn.Sequential(\n",
    "            nn.Linear(h*2,   h),\n",
    "            nn.LayerNorm(h),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(d),\n",
    "            nn.Linear(h, config['num_traits'])\n",
    "        )\n",
    "\n",
    "    def forward(self, emo_input=None, per_input=None):\n",
    "        \n",
    "        base_emo_logits = base_per_scores = None\n",
    "        if emo_input is not None:\n",
    "            x_e, len_e = emo_input\n",
    "            feat_e = self.emo_enc.extract_features(x_e, len_e)   \n",
    "            base_emo_logits = self.emo_enc(x_e, len_e)         \n",
    "            emo_emd = self.emo_proj(feat_e)                    \n",
    "        if per_input is not None:\n",
    "            x_p, len_p = per_input\n",
    "            feat_p = self.per_enc.extract_features(x_p, len_p) \n",
    "            base_per_scores = self.per_enc(x_p, len_p)          \n",
    "            per_emd = self.per_proj(feat_p)                    \n",
    "            \n",
    "        if emo_input is not None and per_input is not None:\n",
    "           \n",
    "            attn_e2p, _ = self.mha_e2p(query=emo_emd, key=per_emd, value=per_emd)\n",
    "            emo_emd = emo_emd + attn_e2p\n",
    "          \n",
    "            attn_p2e, _ = self.mha_p2e(query=per_emd, key=emo_emd, value=emo_emd)\n",
    "            per_emd = per_emd + attn_p2e\n",
    "\n",
    "            fe = emo_emd.mean(dim=1) \n",
    "            fp = per_emd.mean(dim=1) \n",
    "            cat = torch.cat([fe, fp], dim=-1) \n",
    "            \n",
    "            emo_new = self.emo_head(cat)   \n",
    "            per_new = self.per_head(cat)\n",
    "            \n",
    "            final_emo = (emo_new + base_emo_logits) / 2\n",
    "            final_per = (per_new + base_per_scores) / 2\n",
    "\n",
    "            return {'emotion': final_emo, 'personality': final_per}\n",
    "\n",
    "        elif emo_input is not None:\n",
    "            return {'emotion': base_emo_logits}\n",
    "        else:\n",
    "            return {'personality': base_per_scores}\n",
    "\n",
    "def load_pretrained_emotion_encoder(path, device):\n",
    "    enc = CustomMambaClassifier(input_size=1024, d_model=256, num_layers=3, num_classes=7, dropout=0.2).to(device)\n",
    "    ck = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(ck['model_state_dict'])\n",
    "    enc.output_dim = 256\n",
    "\n",
    "    def extract_features(x, lengths):\n",
    "        h = enc.input_proj(x)               \n",
    "        for blk in enc.blocks:\n",
    "            h = blk(h)                       \n",
    "        return h\n",
    "\n",
    "    enc.extract_features = extract_features\n",
    "    enc.eval()\n",
    "    return enc\n",
    "\n",
    "def load_pretrained_personality_encoder(path, device):\n",
    "    enc = CustomMambaRegressor(input_size=1024, d_model=256, num_layers=3, num_targets=5, dropout=0.2).to(device)\n",
    "    ck = torch.load(path, map_location=device)\n",
    "    enc.load_state_dict(ck)\n",
    "    enc.output_dim = 256\n",
    "\n",
    "    def extract_features(x, lengths):\n",
    "        h = enc.input_proj(x)               \n",
    "        for blk in enc.blocks:\n",
    "            h = blk(h)                       \n",
    "        return h\n",
    "\n",
    "    enc.extract_features = extract_features\n",
    "    enc.eval()\n",
    "    return enc\n",
    "\n",
    "\n",
    "def load_fusion_model(\n",
    "    fusion_ckpt_path: str,\n",
    "    emo_encoder_ckpt: str,\n",
    "    per_encoder_ckpt: str,\n",
    "    device: str = 'cpu'\n",
    "):\n",
    "    device = torch.device(device)\n",
    "    emo_enc = load_pretrained_emotion_encoder(emo_encoder_ckpt, device)\n",
    "    per_enc = load_pretrained_personality_encoder(per_encoder_ckpt, device)\n",
    "    ckpt       = torch.load(fusion_ckpt_path, map_location=device)\n",
    "    best_cfg   = ckpt['config']\n",
    "    state_dict = ckpt['state_dict']\n",
    "    model = FusionTransformer(emo_enc, per_enc, best_cfg).to(device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    return model, device\n",
    "\n",
    "def run_inference(\n",
    "    model: FusionTransformer,\n",
    "    device: torch.device,\n",
    "    embedding: np.ndarray\n",
    "):\n",
    "    if embedding.ndim == 3 and embedding.shape[0] == 1:\n",
    "        emb = embedding[0]  \n",
    "    elif embedding.ndim == 2:\n",
    "        emb = embedding    \n",
    "\n",
    "    x = torch.tensor(emb, dtype=torch.float32).unsqueeze(0).to(device) \n",
    "    lengths = torch.tensor([emb.shape[0]], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(emo_input=(x, lengths), per_input=(x, lengths))\n",
    "        emo_logits = out['emotion'].cpu().numpy().squeeze(0)\n",
    "        per_scores = out['personality'].cpu().numpy().squeeze(0)\n",
    "        emo_probs  = torch.softmax(torch.tensor(emo_logits), dim=-1).numpy()\n",
    "    return emo_probs, per_scores\n",
    "\n",
    "def extract_embeddings(audio_path: str) -> np.ndarray:\n",
    "        signal, sr = librosa.load(audio_path, sr=16000)\n",
    "        emb = process_audio(signal, sr)\n",
    "        return emb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FUSION_CKPT = \"best_fusion_overall_mamba.pt\"\n",
    "    EMO_ENC_CKPT = \"final_best_model_uni_mamba.pt\"\n",
    "    PER_ENC_CKPT = \"best_mamba_regressor.pth\"\n",
    "\n",
    "    model, device = load_fusion_model(\n",
    "        FUSION_CKPT, EMO_ENC_CKPT, PER_ENC_CKPT, device='cpu'\n",
    "    )\n",
    "    \n",
    "    emo_names = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise']\n",
    "    pers_names = ['Openness', 'Conscientiousness', 'Extraversion', 'Agreeableness', 'Neuroticism']\n",
    "    \n",
    "    emb = extract_embeddings(\"your_audio.wav\")\n",
    "    emo_probs, per_scores = run_inference(model, device, emb)\n",
    "    print(\"Emotion\")\n",
    "    for name, v in zip(emo_names, emo_probs):\n",
    "        print(f\"  {name}: {v:.4f}\")\n",
    "        \n",
    "    print(\"Personality\")   \n",
    "    for name, v in zip(pers_names, per_scores):\n",
    "        print(f\"  {name}: {v:.4f}\")\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-05T02:33:16.859601Z",
     "start_time": "2025-07-05T02:33:14.667493Z"
    }
   },
   "id": "f09c2663ddccab51",
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
